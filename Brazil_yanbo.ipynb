{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import os\n",
    "import gc\n",
    "import sklearn.model_selection\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import lightgbm as lgb\n",
    "\n",
    "## suppress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process Whole Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path = \"/export/storage_cpoliqui/data/raw/wages2008.tsv\"\n",
    "data = pd.read_csv(csv_path, sep = '\\t', header = 0)\n",
    "\n",
    "header_raw = [\"Firm_Identifier\", \"Establishment_Identifier\", \"Year\", \"emp_present_12_31\",\n",
    "              \"Worker_Identifier\", \"Avg_Month_Wage_In_miniWage\", \"contract_salary_rea\", \"salary_type\",\n",
    "              \"Weekly_hrs\", \"Occupation_code\", \"reason_leave\", \"How_worker_entered\", \"date_enter\",\n",
    "              \"date_left\", \"employment_type\", \"tenure_in_month\", \"female\", \"Education_level\", \"nationality\",\n",
    "              \"Race\", \"Age\", \"municipality\", \"Microregion\", \"Corp_Form\", \"Industry_cnae\"]\n",
    "\n",
    "data.columns = header_raw\n",
    "\n",
    "def transform_sal(sal_type, Weekly_hrs, contract_salary_rea):\n",
    "    map_a = {1:4.333, 2:2.1667, 3:1, 4:0.2, 5:(1 / Weekly_hrs)}\n",
    "    return contract_salary_rea / (Weekly_hrs * map_a[sal_type])\n",
    "\n",
    "v_transform_sal = np.vectorize(transform_sal)\n",
    "\n",
    "def process_data(data_1):\n",
    "    data_1.drop(columns =\"Microregion\",inplace=True)\n",
    "    data_1.drop(columns =\"date_left\",inplace=True)\n",
    "    data_1.drop(columns =\"date_enter\",inplace=True)\n",
    "    mode_race = data_1[\"Race\"].mode()\n",
    "    data_1[\"Race\"].fillna(mode_race, inplace=True)\n",
    "    data_1.dropna(inplace = True)\n",
    "    \n",
    "    data_1[\"contract_salary_rea\"]=data_1[\"contract_salary_rea\"]/100\n",
    "    data_1=data_1.query('salary_type<=5')\n",
    "    data_1=data_1.loc[(data_1[\"Firm_Identifier\"]>=10)&(data_1[\"Race\"]!=9)]\n",
    "    data_1=data_1.loc[(data_1[\"contract_salary_rea\"]>0)&(data_1[\"Avg_Month_Wage_In_miniWage\"]>0)]   \n",
    "    data_1[\"hr_salary\"]=v_transform_sal(data_1[\"salary_type\"].values,data_1[\"Weekly_hrs\"].values,data_1[\"contract_salary_rea\"].values)\n",
    "    data_1.loc[:,\"hr_salary_log\"]=np.log(data_1[\"hr_salary\"])\n",
    "    \n",
    "    #get rid of a small portion of entries where log(w)<0\n",
    "    data_1=data_1.loc[(data_1[\"hr_salary_log\"]>=0)]\n",
    "    data_1=data_1.reset_index(drop=True)\n",
    "    \n",
    "    return data_1\n",
    "\n",
    "data = process_data(data) # whole data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split single-market firms and multi-market firms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set aside multi market firm from single market firm\n",
    "def Single_Multiple(data_1):\n",
    "    a=data_1.groupby([\"Firm_Identifier\"])[\"municipality\"].nunique()\n",
    "    single_market_id=a.loc[a==1]\n",
    "    multi_market_id=a.loc[a!=1]\n",
    "    multi_market_id=pd.Series(multi_market_id.index)\n",
    "    single_market_id=pd.Series(single_market_id.index)\n",
    "\n",
    "    single_market_firm=data_1.loc[data_1['Firm_Identifier'].isin(single_market_id)]\n",
    "    multi_market_firm=data_1.loc[data_1['Firm_Identifier'].isin(multi_market_id)]\n",
    "    return single_market_firm, multi_market_firm\n",
    "\n",
    "single_whole, multi_whole = Single_Multiple(data) # whole data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For single-market firms, split a 10% training set and a 90% testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (3016748, 24) Test: (27150734, 24)\n"
     ]
    }
   ],
   "source": [
    "def get_train_test(data_1):\n",
    "    train, test = sklearn.model_selection.train_test_split(data_1, train_size=0.1,\n",
    "                                                            test_size=0.9, random_state=42)\n",
    "    print('Train:', train.shape, 'Test:', test.shape)    \n",
    "    train.reset_index(drop=True,inplace=True)\n",
    "    test.reset_index(drop=True,inplace=True)\n",
    "    return train, test\n",
    "\n",
    "single_train, single_test = get_train_test(single_whole)\n",
    "\n",
    "def design_matrix(t):\n",
    "    result = t.drop(columns=[\"Year\", \"Worker_Identifier\", \"Avg_Month_Wage_In_miniWage\",\n",
    "                    \"contract_salary_rea\", \"hr_salary\", \"emp_present_12_31\"])\n",
    "    result.reset_index(drop=True,inplace=True)\n",
    "    return result\n",
    "\n",
    "def rmse(pred, actual):\n",
    "    return np.sqrt(np.mean((actual - pred) ** 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform data into LightGBM format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_train = design_matrix(single_train)\n",
    "multi_whole = design_matrix(multi_whole)\n",
    "single_test = design_matrix(single_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find the best set of hyper-parameters for the LightGBM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_lgb(data,                   ## data input, including all predictors and the outcome\n",
    "             x_var,                  ## an array predictor names\n",
    "             y_var,                  ## outcome name\n",
    "             param_arr,              ## an array of hyper-parameter sets, in the form of dictionaries\n",
    "             k_fold = 10,            ## the k for k-fold cross-validation\n",
    "             test_size = 0.33,       ## size pf the test size for cross-validation\n",
    "             random_state = 127,     ## set a random seed for the train-test splitting\n",
    "             verbose_eval = False):  ## False to turn off log output when fitting the LightGBM\n",
    "    \n",
    "    ## import required libraries\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import lightgbm as lgb\n",
    "    import sklearn.model_selection\n",
    "    \n",
    "    def rmse(pred, actual):\n",
    "        return np.sqrt(np.mean((actual - pred) ** 2))\n",
    "    \n",
    "    \n",
    "    ## seperate predictors(x) and outcome(y)\n",
    "    ## if no specific columns names for x, then use all variables\n",
    "    if len(x_var) == 0:\n",
    "        x = data.drop(columns = y_var)\n",
    "        x.reset_index(drop = True, inplace = True)\n",
    "    else:\n",
    "        x = data[x_var].drop(columns = y_var)\n",
    "        x.reset_index(drop = True, inplace = True)\n",
    "    \n",
    "    y = data[y_var]\n",
    "\n",
    "    mean_rmse = []\n",
    "    params_final = []\n",
    "    \n",
    "    ## test each set of input hyper-parameteres\n",
    "    for par in param_arr:\n",
    "        \n",
    "        params = {'boosting_type': 'gbdt',\n",
    "                  'objective': 'regression',\n",
    "                  'metric': 'rmse',\n",
    "                  'max_depth': -1,\n",
    "                  'learning_rate': 0.1,\n",
    "                  'early_stopping_round': 20,\n",
    "                  'num_leaves': 50,\n",
    "                  'verbose': -1,\n",
    "                  'num_iterations': 10000}\n",
    "        \n",
    "        params.update(par)\n",
    "        params_final.append(params)\n",
    "        \n",
    "        iter_rmse = []\n",
    "        \n",
    "        ## k-fold cross-validation\n",
    "        for i in range(0, k_fold - 1):\n",
    "            \n",
    "            x_train, x_test, y_train, y_test = sklearn.model_selection.train_test_split(x, y,\n",
    "                                                                       test_size = test_size,\n",
    "                                                                       random_state = random_state + i)\n",
    "            ## tranform data into LightGBM format\n",
    "            x_train_lgb = lgb.Dataset(x_train, label = y_train)\n",
    "            \n",
    "            ## fit the model using the training set\n",
    "            model = lgb.train(params, x_train_lgb, valid_sets = x_train_lgb,\n",
    "                              verbose_eval = verbose_eval)\n",
    "            \n",
    "            ## predict using the testing set\n",
    "            pred = model.predict(x_test)\n",
    "            \n",
    "            ## store the testing RMSE\n",
    "            iter_rmse.append(rmse(pred.reshape(1, -1)[0], y_test))\n",
    "        \n",
    "        mean_rmse.append(np.mean(iter_rmse))\n",
    "    \n",
    "    ind = mean_rmse.index(np.min(mean_rmse))  ## the index of the smallest average RMSE\n",
    "    \n",
    "    print(\"A minimum mean testing RMSE of\", mean_rmse[ind], \"is given by parameter set\", ind + 1)\n",
    "    print(param_arr[ind])\n",
    "    \n",
    "    return params_final[ind] ## return a complete set of the best hyper-parameters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try some arbitury sets of hyper-parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the best hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A minimum mean testing RMSE of 0.33897898369152873 is given by parameter set 8\n",
      "{'learning_rate': 0.1, 'num_leaves': 70}\n",
      "{'boosting_type': 'gbdt', 'objective': 'regression', 'metric': 'rmse', 'max_depth': -1, 'learning_rate': 0.1, 'early_stopping_round': 20, 'num_leaves': 70, 'verbose': -1, 'num_iterations': 10000}\n"
     ]
    }
   ],
   "source": [
    "params1 = {'learning_rate': 0.05, \"num_leaves\": 30}\n",
    "params2 = {'learning_rate': 0.1, \"num_leaves\": 30}\n",
    "params3 = {'learning_rate': 0.2, \"num_leaves\": 30}\n",
    "params4 = {'learning_rate': 0.05, \"num_leaves\": 50}\n",
    "params5 = {'learning_rate': 0.1, \"num_leaves\": 50}\n",
    "params6 = {'learning_rate': 0.2, \"num_leaves\": 50}\n",
    "params7 = {'learning_rate': 0.05, \"num_leaves\": 70}\n",
    "params8 = {'learning_rate': 0.1, \"num_leaves\": 70}\n",
    "params9 = {'learning_rate': 0.2, \"num_leaves\": 70}\n",
    "\n",
    "\n",
    "best_params = tune_lgb(data = single_train, x_var = [], y_var = \"hr_salary_log\",\n",
    "                       param_arr = [params1,params2,params3,params4,params5,params6,params7,params8,params9],\n",
    "                       k_fold = 2)\n",
    "\n",
    "print(best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit the model with the 10% training set of single-market firms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_train_lgb = lgb.Dataset(single_train, label = single_train[\"hr_salary_log\"])\n",
    "\n",
    "model_lgb = lgb.train(best_params, single_train_lgb, valid_sets = single_train_lgb,\n",
    "                      verbose_eval = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict using the 90% testing set of single-market firms and the entire multi-market firms data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.030469906518112218\n",
      "0.049330518095784266\n"
     ]
    }
   ],
   "source": [
    "pred_single = model_lgb.predict(single_test).reshape(1, -1)   ## single-market firms, 90% testing set\n",
    "pred_multi = model_lgb.predict(multi_whole).reshape(1, -1)    ## multi-market firms, entire data\n",
    "\n",
    "\n",
    "print(rmse(pred_single[0], single_test[\"hr_salary_log\"]))\n",
    "\n",
    "print(rmse(pred_multi[0], multi_whole[\"hr_salary_log\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
